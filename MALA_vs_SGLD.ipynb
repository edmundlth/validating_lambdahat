{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn2xLeDc4kuP4nv1MIxL3V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDBRd7na8gaV",
        "outputId": "edb252c7-59ed-450e-9185-d7f1e448aac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.11-py3-none-any.whl (370 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.0/371.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpyro\n",
            "  Downloading numpyro-0.13.2-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.7/312.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (1.4.0)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku)\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (1.23.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (0.9.0)\n",
            "Requirement already satisfied: flax>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (0.7.5)\n",
            "Requirement already satisfied: jax>=0.4.14 in /usr/local/lib/python3.10/dist-packages (from numpyro) (0.4.23)\n",
            "Requirement already satisfied: jaxlib>=0.4.14 in /usr/local/lib/python3.10/dist-packages (from numpyro) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from numpyro) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from numpyro) (4.66.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (0.1.8)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (13.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.14->numpyro) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.14->numpyro) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.14->numpyro) (1.11.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.1->dm-haiku) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.1->dm-haiku) (2.16.1)\n",
            "Requirement already satisfied: chex>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.1->dm-haiku) (0.1.7)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (1.6.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (3.20.3)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax>=0.7.1->dm-haiku) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax>=0.7.1->dm-haiku) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.1->dm-haiku) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (3.17.0)\n",
            "Installing collected packages: jmp, numpyro, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.11 jmp-0.0.4 numpyro-0.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install dm-haiku numpyro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence, NamedTuple\n",
        "import copy, time, json\n",
        "\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, HMC, NUTS\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as jtree\n",
        "import haiku as hk\n",
        "import optax"
      ],
      "metadata": {
        "id": "32vR55Bu-RbE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGLDConfig(NamedTuple):\n",
        "  epsilon: float\n",
        "  gamma: float\n",
        "  num_steps: int\n",
        "\n",
        "def generate_rngkey_tree(key_or_seed, tree_or_treedef):\n",
        "    rngseq = hk.PRNGSequence(key_or_seed)\n",
        "    return jtree.tree_map(lambda _: next(rngseq), tree_or_treedef)\n",
        "\n",
        "def optim_sgld(epsilon, rngkey_or_seed, preconditioning=None):\n",
        "    @jax.jit\n",
        "    def sgld_delta(g, rngkey, p=1):\n",
        "        eta = jax.random.normal(rngkey, shape=g.shape) * jnp.sqrt(epsilon * p)\n",
        "        return -epsilon * p * g / 2 + eta\n",
        "\n",
        "    def init_fn(_):\n",
        "        return rngkey_or_seed\n",
        "\n",
        "    if preconditioning is None:\n",
        "        update_map = lambda grads, rngkey_tree: jax.tree_map(sgld_delta, grads, rngkey_tree)\n",
        "    else:\n",
        "        update_map = lambda grads, rngkey_tree: jax.tree_map(sgld_delta, grads, rngkey_tree, preconditioning)\n",
        "\n",
        "    @jax.jit\n",
        "    def update_fn(grads, state):\n",
        "        rngkey, new_rngkey = jax.random.split(state)\n",
        "        rngkey_tree = generate_rngkey_tree(rngkey, grads)\n",
        "        updates = update_map(grads, rngkey_tree)\n",
        "        return updates, new_rngkey\n",
        "    return optax.GradientTransformation(init_fn, update_fn)\n",
        "\n",
        "\n",
        "def create_local_logposterior(avgnegloglikelihood_fn, num_training_data, w_init, gamma, itemp):\n",
        "    def helper(x, y):\n",
        "        return jnp.sum((x - y)**2)\n",
        "\n",
        "    def _logprior_fn(w):\n",
        "        sqnorm = jax.tree_util.tree_map(helper, w, w_init)\n",
        "        return jax.tree_util.tree_reduce(lambda a,b: a + b, sqnorm)\n",
        "\n",
        "    def logprob(w, x, y):\n",
        "        loglike = -num_training_data * avgnegloglikelihood_fn(w, x, y)\n",
        "        logprior = -gamma / 2 * _logprior_fn(w)\n",
        "        return itemp * loglike + logprior\n",
        "    return logprob\n"
      ],
      "metadata": {
        "id": "WXYgo7Mj-VfA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mala_acceptance_probability(current_point, proposed_point, loss_and_grad_fn, step_size):\n",
        "    \"\"\"\n",
        "    Calculate the acceptance probability for a MALA transition.\n",
        "\n",
        "    Args:\n",
        "    current_point: The current point in parameter space.\n",
        "    proposed_point: The proposed point in parameter space.\n",
        "    loss_and_grad_fn (function): Function to compute loss and loss gradient at a point.\n",
        "    step_size (float): Step size parameter for MALA.\n",
        "\n",
        "    Returns:\n",
        "    float: Acceptance probability for the proposed transition.\n",
        "    \"\"\"\n",
        "    # Compute the gradient of the loss at the current point\n",
        "    current_loss, current_grad = loss_and_grad_fn(current_point)\n",
        "    proposed_loss, proposed_grad = loss_and_grad_fn(proposed_point)\n",
        "\n",
        "    # Compute the log of the proposal probabilities (using the Gaussian proposal distribution)\n",
        "    log_q_proposed_to_current = -jnp.sum((current_point - proposed_point - (step_size * 0.5 * -proposed_grad)) ** 2) / (2 * step_size)\n",
        "    log_q_current_to_proposed = -jnp.sum((proposed_point - current_point - (step_size * 0.5 * -current_grad)) ** 2) / (2 * step_size)\n",
        "\n",
        "    # Compute the acceptance probability\n",
        "    acceptance_log_prob = log_q_proposed_to_current - log_q_current_to_proposed + current_loss - proposed_loss\n",
        "    return jnp.minimum(1.0, jnp.exp(acceptance_log_prob))\n",
        "\n",
        "# Example usage:\n",
        "# Define a simple log likelihood function for demonstration\n",
        "def log_likelihood_example(x):\n",
        "    return -0.5 * jnp.sum(x**2)\n",
        "\n",
        "loss_and_grad_fn = jax.jit(jax.value_and_grad(lambda x: -log_likelihood_example(x), argnums=0))\n",
        "\n",
        "# Current and proposed points for demonstration\n",
        "current_point = jnp.array([1.0, 2.0])\n",
        "proposed_point = jnp.array([1.5, 2.5])\n",
        "step_size = 0.1\n",
        "\n",
        "# Calculate the acceptance probability\n",
        "acceptance_prob = mala_acceptance_probability(current_point, proposed_point, loss_and_grad_fn, step_size)\n",
        "acceptance_prob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQjjcq9R-XOt",
        "outputId": "663e6b2a-15b0-4631-e1be-6ca63d2e4a96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.95719343, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pack_params(params):\n",
        "    params_flat, treedef = jax.tree_util.tree_flatten(params)\n",
        "    shapes = [p.shape for p in params_flat]\n",
        "    indices = np.cumsum([p.size for p in params_flat])\n",
        "    params_packed = jnp.concatenate([jnp.ravel(p) for p in params_flat])\n",
        "    pack_info = (treedef, shapes, indices)\n",
        "    return params_packed, pack_info\n",
        "\n",
        "def unpack_params(params_packed, pack_info):\n",
        "    treedef, shapes, indices = pack_info\n",
        "    params_split = jnp.split(params_packed, indices)\n",
        "    params_flat = [jnp.reshape(p, shape) for p, shape in zip(params_split, shapes)]\n",
        "    params = jax.tree_util.tree_unflatten(treedef, params_flat)\n",
        "    return params"
      ],
      "metadata": {
        "id": "vC1JcmTI-b3U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLPs"
      ],
      "metadata": {
        "id": "y1hwXpoc-lVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReluNetwork(hk.Module):\n",
        "    def __init__(self, layer_widths):\n",
        "        super().__init__()\n",
        "        self.layer_widths = layer_widths\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for width in self.layer_widths[:-1]:\n",
        "            x = hk.Linear(width)(x)\n",
        "            x = jax.nn.relu(x)\n",
        "        x = hk.Linear(self.layer_widths[-1])(x)\n",
        "        return x\n",
        "\n",
        "# Function to initialize and apply the DLN model\n",
        "def forward_fn(x, layer_widths):\n",
        "    net = ReluNetwork(layer_widths)\n",
        "    return net(x)\n",
        "\n",
        "# Create a Haiku-transformed version of the model\n",
        "def create_model(layer_widths):\n",
        "    model = hk.without_apply_rng(hk.transform(lambda x: forward_fn(x, layer_widths)))\n",
        "    return model\n",
        "\n",
        "def generate_training_data(true_param, model, input_dim, num_samples):\n",
        "    inputs = np.random.uniform(-10, 10, size=(num_samples, input_dim))\n",
        "\n",
        "    # Apply the true model to generate outputs\n",
        "    true_outputs = model.apply(true_param, inputs)\n",
        "\n",
        "    return inputs, true_outputs\n",
        "\n",
        "def mse_loss(param, model, inputs, targets):\n",
        "    predictions = model.apply(param, inputs)\n",
        "    return jnp.mean((predictions - targets) ** 2)\n",
        "\n",
        "def create_minibatches(inputs, targets, batch_size, num_iter=None, shuffle=True):\n",
        "    assert len(inputs) == len(targets)\n",
        "\n",
        "    if num_iter is None:\n",
        "        num_iter = len(inputs)\n",
        "\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    else:\n",
        "        indices = np.arange(len(inputs))\n",
        "\n",
        "    iter = 0\n",
        "    while True:\n",
        "        for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
        "            excerpt = indices[start_idx:start_idx + batch_size]\n",
        "            yield inputs[excerpt], targets[excerpt]\n",
        "\n",
        "            iter += 1\n",
        "            if iter >= num_iter:\n",
        "                return\n",
        "\n"
      ],
      "metadata": {
        "id": "A1sEBCuI-nDS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 10  # Dimension of the input\n",
        "layer_widths = [20, 20, 10]\n",
        "\n",
        "# Create the model\n",
        "model = create_model(layer_widths)\n",
        "\n",
        "# Initialize the \"true\" model parameters\n",
        "rngkey = jax.random.PRNGKey(0)\n",
        "dummy_input = jnp.zeros((1, input_dim), dtype=jnp.float32)\n",
        "rngkey, subkey = jax.random.split(rngkey)\n",
        "true_param = model.init(rngkey, dummy_input)\n",
        "\n",
        "# Set biases to not be zero\n",
        "for i in range(len(layer_widths)-1):\n",
        "    rngkey, subkey = jax.random.split(rngkey)\n",
        "    loc = f'relu_network/linear_{i}' if i > 0 else 'relu_network/linear'\n",
        "    true_param[loc]['b'] = jax.random.normal(subkey, (layer_widths[i],))\n"
      ],
      "metadata": {
        "id": "Kg_v5p4HPNRo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 1e-5\n",
        "gamma = 1.0\n",
        "batch_size = 32\n",
        "num_samples = 10000\n",
        "num_runs = 5\n",
        "\n",
        "# Sweep over different training data size\n",
        "dataset_sizes = np.round(10**np.linspace(2, 5, 7)).astype(np.int64)\n",
        "sgld_times = []\n",
        "mala_times = []\n",
        "memories = []\n",
        "sgld_lambdas = []\n",
        "mala_lambdas = []\n",
        "for num_training_data in dataset_sizes:\n",
        "    sgld_times.append([])\n",
        "    mala_times.append([])\n",
        "    memories.append([])\n",
        "    sgld_lambdas.append([])\n",
        "    mala_lambdas.append([])\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(f\"Training set size: {num_training_data}, run {run}\")\n",
        "\n",
        "        # Generate training data\n",
        "        np.random.seed(0)\n",
        "        x_train, y_train = generate_training_data(true_param, model, input_dim, num_training_data)\n",
        "        print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        jtree.tree_map_with_path(lambda path, x: x.shape, true_param)\n",
        "\n",
        "        memory = (x_train.size * x_train.itemsize) + (y_train.size * y_train.itemsize)\n",
        "        memories[-1].append(memory)\n",
        "        print(\"Training data memory: \", memory)\n",
        "\n",
        "        itemp = 1 / np.log(num_training_data)\n",
        "        param_init = copy.deepcopy(true_param)\n",
        "\n",
        "        loss_fn = jax.jit(lambda param, inputs, targets: mse_loss(param, model, inputs, targets))\n",
        "        local_logprob = create_local_logposterior(\n",
        "                avgnegloglikelihood_fn=loss_fn,\n",
        "                num_training_data=num_training_data,\n",
        "                w_init=param_init,\n",
        "                gamma=gamma,\n",
        "                itemp=itemp,\n",
        "            )\n",
        "\n",
        "        # Run SGLD\n",
        "        sgld_start_time = time.time()\n",
        "\n",
        "        sgld_grad_fn = jax.jit(jax.value_and_grad(lambda w, x, y: -local_logprob(w, x, y), argnums=0))\n",
        "\n",
        "        rngkey = jax.random.PRNGKey(run)\n",
        "        sgldoptim = optim_sgld(step_size, rngkey)\n",
        "        samples = []\n",
        "        losses = []\n",
        "        accept_probs = []\n",
        "        opt_state = sgldoptim.init(param_init)\n",
        "        param = param_init\n",
        "        t = 0\n",
        "        for x_batch, y_batch in create_minibatches(x_train, y_train, batch_size=batch_size, num_iter=num_samples):\n",
        "            old_param = param.copy()\n",
        "\n",
        "            nll, grads = sgld_grad_fn(param, x_batch, y_batch)\n",
        "            updates, opt_state = sgldoptim.update(grads, opt_state)\n",
        "            param = optax.apply_updates(param, updates)\n",
        "            samples.append(param)\n",
        "            losses.append(loss_fn(param, x_batch, y_batch))\n",
        "\n",
        "            t += 1\n",
        "\n",
        "            if t % 20 == 0:\n",
        "                old_param_packed, pack_info = pack_params(old_param)\n",
        "                param_packed, _ = pack_params(param)\n",
        "                def grad_fn_packed(w):\n",
        "                    nll, grad = sgld_grad_fn(unpack_params(w, pack_info), x_batch, y_batch)\n",
        "                    grad_packed, _ = pack_params(grad)\n",
        "                    return nll, grad_packed\n",
        "                accept_probs.append(mala_acceptance_probability(\n",
        "                    old_param_packed, param_packed, grad_fn_packed, step_size))\n",
        "\n",
        "        init_loss = loss_fn(param_init, x_train, y_train)\n",
        "        lambdahat = ((np.mean(losses) - init_loss) * num_training_data * itemp).item()\n",
        "\n",
        "        sgld_time = time.time() - sgld_start_time\n",
        "\n",
        "        sgld_lambdas[-1].append(lambdahat)\n",
        "        sgld_times[-1].append(sgld_time)\n",
        "\n",
        "        print(\"SGLD lambda: \", lambdahat)\n",
        "        print(\"SGLD accept prob: \", np.mean(accept_probs))\n",
        "        print(\"SGLD execution time: \", sgld_time)\n",
        "\n",
        "        # Run MALA\n",
        "        mala_start_time = time.time()\n",
        "\n",
        "        rngkey = jax.random.PRNGKey(run)\n",
        "\n",
        "        param_init = copy.deepcopy(true_param)\n",
        "        beta = itemp*num_training_data\n",
        "        mala_step_size = np.sqrt(step_size)\n",
        "        num_steps = 1\n",
        "\n",
        "        loss_fn = jax.jit(lambda w: mse_loss(w, model, x_train, y_train))\n",
        "        potential_fn = jax.jit(lambda w: -local_logprob(w, x_train, y_train))\n",
        "\n",
        "        hmc_kernel = HMC(\n",
        "            #potential_fn=lambda param: beta * loss_fn(param),\n",
        "            potential_fn=potential_fn,\n",
        "            step_size=mala_step_size,\n",
        "            trajectory_length=mala_step_size*num_steps,\n",
        "            adapt_step_size=False,\n",
        "            adapt_mass_matrix=False\n",
        "        )\n",
        "        mcmc = MCMC(hmc_kernel, num_samples=10000, num_warmup=0)\n",
        "        rngkey, subkey = jax.random.split(rngkey)\n",
        "        mcmc.run(subkey, init_params=param_init)\n",
        "\n",
        "        # Extract samples\n",
        "        samples = mcmc.get_samples()\n",
        "\n",
        "        init_loss = loss_fn(param_init)\n",
        "        losses = (jax.lax.map(loss_fn, samples))\n",
        "        lambdahat = ((np.mean(losses) - init_loss) * beta).item()\n",
        "\n",
        "        mala_time = time.time() - mala_start_time\n",
        "\n",
        "        mala_lambdas[-1].append(lambdahat)\n",
        "        mala_times[-1].append(mala_time)\n",
        "\n",
        "        print(\"MALA lambda: \", lambdahat)\n",
        "        print(\"MALA execution time: \", mala_time)\n",
        "\n",
        "# Save results to JSON\n",
        "results = {\n",
        "    \"dataset_sizes\": dataset_sizes.tolist(),\n",
        "    \"SGLD\": {\n",
        "        \"lambdas\": sgld_lambdas,\n",
        "        \"times\": sgld_times\n",
        "    },\n",
        "    \"MALA\": {\n",
        "        \"lambdas\": mala_lambdas,\n",
        "        \"times\": mala_times\n",
        "    },\n",
        "}\n",
        "json.dump(results, open(\"mala_v_sgld_results.json\", \"w\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crUy9RnoJUgl",
        "outputId": "7bf620e1-9747-4036-bdb3-9527171473cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 100, run 0\n",
            "x_train shape: (100, 10), y_train shape: (100, 10)\n",
            "Training data memory:  12000\n",
            "SGLD lambda:  57.37425994873047\n",
            "SGLD accept prob:  0.99507684\n",
            "SGLD execution time:  33.13397693634033\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:20<00:00, 31.23it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MALA lambda:  50.44807434082031\n",
            "MALA execution time:  322.0246331691742\n",
            "Training set size: 100, run 1\n",
            "x_train shape: (100, 10), y_train shape: (100, 10)\n",
            "Training data memory:  12000\n",
            "SGLD lambda:  57.60322952270508\n",
            "SGLD accept prob:  0.9949707\n",
            "SGLD execution time:  31.66882586479187\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:16<00:00, 31.60it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MALA lambda:  56.770263671875\n",
            "MALA execution time:  317.98200130462646\n",
            "Training set size: 100, run 2\n",
            "x_train shape: (100, 10), y_train shape: (100, 10)\n",
            "Training data memory:  12000\n",
            "SGLD lambda:  55.59379577636719\n",
            "SGLD accept prob:  0.99578637\n",
            "SGLD execution time:  32.193074226379395\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:20<00:00, 31.18it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MALA lambda:  53.56113815307617\n",
            "MALA execution time:  321.9629626274109\n",
            "Training set size: 100, run 3\n",
            "x_train shape: (100, 10), y_train shape: (100, 10)\n",
            "Training data memory:  12000\n",
            "SGLD lambda:  52.4981689453125\n",
            "SGLD accept prob:  0.9961521\n",
            "SGLD execution time:  33.36083197593689\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:20<00:00, 31.21it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MALA lambda:  58.9940185546875\n",
            "MALA execution time:  321.6508927345276\n",
            "Training set size: 100, run 4\n",
            "x_train shape: (100, 10), y_train shape: (100, 10)\n",
            "Training data memory:  12000\n",
            "SGLD lambda:  51.65175247192383\n",
            "SGLD accept prob:  0.99534637\n",
            "SGLD execution time:  31.509000062942505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:21<00:00, 31.07it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  51.5749397277832\n",
            "MALA execution time:  323.8936080932617\n",
            "Training set size: 316, run 0\n",
            "x_train shape: (316, 10), y_train shape: (316, 10)\n",
            "Training data memory:  37920\n",
            "SGLD lambda:  90.11341857910156\n",
            "SGLD accept prob:  0.9918604\n",
            "SGLD execution time:  32.32085371017456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:26<00:00, 30.61it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  82.34846496582031\n",
            "MALA execution time:  328.3618640899658\n",
            "Training set size: 316, run 1\n",
            "x_train shape: (316, 10), y_train shape: (316, 10)\n",
            "Training data memory:  37920\n",
            "SGLD lambda:  88.00234985351562\n",
            "SGLD accept prob:  0.992213\n",
            "SGLD execution time:  31.303765296936035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:33<00:00, 46.86it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  88.04817199707031\n",
            "MALA execution time:  214.82731652259827\n",
            "Training set size: 316, run 2\n",
            "x_train shape: (316, 10), y_train shape: (316, 10)\n",
            "Training data memory:  37920\n",
            "SGLD lambda:  87.2174301147461\n",
            "SGLD accept prob:  0.99178475\n",
            "SGLD execution time:  33.534610748291016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [01:33<00:00, 107.26it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  86.43769073486328\n",
            "MALA execution time:  95.11934041976929\n",
            "Training set size: 316, run 3\n",
            "x_train shape: (316, 10), y_train shape: (316, 10)\n",
            "Training data memory:  37920\n",
            "SGLD lambda:  84.71837615966797\n",
            "SGLD accept prob:  0.99199104\n",
            "SGLD execution time:  31.8625328540802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:31<00:00, 47.18it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  91.58927917480469\n",
            "MALA execution time:  213.74442982673645\n",
            "Training set size: 316, run 4\n",
            "x_train shape: (316, 10), y_train shape: (316, 10)\n",
            "Training data memory:  37920\n",
            "SGLD lambda:  83.54853057861328\n",
            "SGLD accept prob:  0.9935224\n",
            "SGLD execution time:  32.6684365272522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:32<00:00, 47.11it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  83.38666534423828\n",
            "MALA execution time:  213.8702564239502\n",
            "Training set size: 1000, run 0\n",
            "x_train shape: (1000, 10), y_train shape: (1000, 10)\n",
            "Training data memory:  120000\n",
            "SGLD lambda:  140.54531860351562\n",
            "SGLD accept prob:  0.9863273\n",
            "SGLD execution time:  32.47716474533081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:33<00:00, 46.95it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  139.86952209472656\n",
            "MALA execution time:  215.41528844833374\n",
            "Training set size: 1000, run 1\n",
            "x_train shape: (1000, 10), y_train shape: (1000, 10)\n",
            "Training data memory:  120000\n",
            "SGLD lambda:  135.913818359375\n",
            "SGLD accept prob:  0.98634744\n",
            "SGLD execution time:  31.702524423599243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:31<00:00, 47.21it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  141.77679443359375\n",
            "MALA execution time:  214.84855699539185\n",
            "Training set size: 1000, run 2\n",
            "x_train shape: (1000, 10), y_train shape: (1000, 10)\n",
            "Training data memory:  120000\n",
            "SGLD lambda:  138.0520782470703\n",
            "SGLD accept prob:  0.98419017\n",
            "SGLD execution time:  31.822795629501343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:32<00:00, 47.13it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  141.62376403808594\n",
            "MALA execution time:  214.1477770805359\n",
            "Training set size: 1000, run 3\n",
            "x_train shape: (1000, 10), y_train shape: (1000, 10)\n",
            "Training data memory:  120000\n",
            "SGLD lambda:  133.38619995117188\n",
            "SGLD accept prob:  0.98663634\n",
            "SGLD execution time:  32.38638710975647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:32<00:00, 47.13it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  141.63909912109375\n",
            "MALA execution time:  215.22088623046875\n",
            "Training set size: 1000, run 4\n",
            "x_train shape: (1000, 10), y_train shape: (1000, 10)\n",
            "Training data memory:  120000\n",
            "SGLD lambda:  136.6833038330078\n",
            "SGLD accept prob:  0.9862479\n",
            "SGLD execution time:  32.265281200408936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:31<00:00, 47.36it/s, 1 steps of size 3.16e-03. acc. prob=1.00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  135.15985107421875\n",
            "MALA execution time:  213.16329979896545\n",
            "Training set size: 3162, run 0\n",
            "x_train shape: (3162, 10), y_train shape: (3162, 10)\n",
            "Training data memory:  379440\n",
            "SGLD lambda:  216.7156219482422\n",
            "SGLD accept prob:  0.9768807\n",
            "SGLD execution time:  32.4572389125824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:44<00:00, 44.48it/s, 1 steps of size 3.16e-03. acc. prob=0.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  219.56593322753906\n",
            "MALA execution time:  230.1003017425537\n",
            "Training set size: 3162, run 1\n",
            "x_train shape: (3162, 10), y_train shape: (3162, 10)\n",
            "Training data memory:  379440\n",
            "SGLD lambda:  210.0811767578125\n",
            "SGLD accept prob:  0.9786686\n",
            "SGLD execution time:  33.4709038734436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:43<00:00, 44.69it/s, 1 steps of size 3.16e-03. acc. prob=0.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  215.56591796875\n",
            "MALA execution time:  228.94415950775146\n",
            "Training set size: 3162, run 2\n",
            "x_train shape: (3162, 10), y_train shape: (3162, 10)\n",
            "Training data memory:  379440\n",
            "SGLD lambda:  211.4884490966797\n",
            "SGLD accept prob:  0.9793658\n",
            "SGLD execution time:  32.61261057853699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:44<00:00, 44.54it/s, 1 steps of size 3.16e-03. acc. prob=0.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  217.02134704589844\n",
            "MALA execution time:  230.35093355178833\n",
            "Training set size: 3162, run 3\n",
            "x_train shape: (3162, 10), y_train shape: (3162, 10)\n",
            "Training data memory:  379440\n",
            "SGLD lambda:  207.4788055419922\n",
            "SGLD accept prob:  0.9762826\n",
            "SGLD execution time:  32.9333279132843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:41<00:00, 45.07it/s, 1 steps of size 3.16e-03. acc. prob=0.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  217.24588012695312\n",
            "MALA execution time:  227.01857805252075\n",
            "Training set size: 3162, run 4\n",
            "x_train shape: (3162, 10), y_train shape: (3162, 10)\n",
            "Training data memory:  379440\n",
            "SGLD lambda:  210.2639923095703\n",
            "SGLD accept prob:  0.98327565\n",
            "SGLD execution time:  32.28909945487976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [03:42<00:00, 44.92it/s, 1 steps of size 3.16e-03. acc. prob=0.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  207.61856079101562\n",
            "MALA execution time:  227.46654200553894\n",
            "Training set size: 10000, run 0\n",
            "x_train shape: (10000, 10), y_train shape: (10000, 10)\n",
            "Training data memory:  1200000\n",
            "SGLD lambda:  296.7646179199219\n",
            "SGLD accept prob:  0.9770875\n",
            "SGLD execution time:  31.998055458068848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [04:19<00:00, 38.53it/s, 1 steps of size 3.16e-03. acc. prob=0.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  300.5058288574219\n",
            "MALA execution time:  273.6828966140747\n",
            "Training set size: 10000, run 1\n",
            "x_train shape: (10000, 10), y_train shape: (10000, 10)\n",
            "Training data memory:  1200000\n",
            "SGLD lambda:  295.031005859375\n",
            "SGLD accept prob:  0.98025537\n",
            "SGLD execution time:  32.08233976364136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [04:18<00:00, 38.62it/s, 1 steps of size 3.16e-03. acc. prob=0.95]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  293.9537658691406\n",
            "MALA execution time:  272.72760701179504\n",
            "Training set size: 10000, run 2\n",
            "x_train shape: (10000, 10), y_train shape: (10000, 10)\n",
            "Training data memory:  1200000\n",
            "SGLD lambda:  297.7831115722656\n",
            "SGLD accept prob:  0.98122716\n",
            "SGLD execution time:  33.12505316734314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [04:17<00:00, 38.91it/s, 1 steps of size 3.16e-03. acc. prob=0.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  302.910400390625\n",
            "MALA execution time:  269.8833963871002\n",
            "Training set size: 10000, run 3\n",
            "x_train shape: (10000, 10), y_train shape: (10000, 10)\n",
            "Training data memory:  1200000\n",
            "SGLD lambda:  286.37640380859375\n",
            "SGLD accept prob:  0.97771543\n",
            "SGLD execution time:  33.54761815071106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [04:20<00:00, 38.40it/s, 1 steps of size 3.16e-03. acc. prob=0.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  308.1272277832031\n",
            "MALA execution time:  273.35159492492676\n",
            "Training set size: 10000, run 4\n",
            "x_train shape: (10000, 10), y_train shape: (10000, 10)\n",
            "Training data memory:  1200000\n",
            "SGLD lambda:  298.7560119628906\n",
            "SGLD accept prob:  0.9783385\n",
            "SGLD execution time:  32.09901571273804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [04:19<00:00, 38.61it/s, 1 steps of size 3.16e-03. acc. prob=0.94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  275.41015625\n",
            "MALA execution time:  271.9844448566437\n",
            "Training set size: 31623, run 0\n",
            "x_train shape: (31623, 10), y_train shape: (31623, 10)\n",
            "Training data memory:  3794760\n",
            "SGLD lambda:  344.8536071777344\n",
            "SGLD accept prob:  0.98994964\n",
            "SGLD execution time:  33.68674302101135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:46<00:00, 28.90it/s, 1 steps of size 3.16e-03. acc. prob=0.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  355.89202880859375\n",
            "MALA execution time:  382.2019579410553\n",
            "Training set size: 31623, run 1\n",
            "x_train shape: (31623, 10), y_train shape: (31623, 10)\n",
            "Training data memory:  3794760\n",
            "SGLD lambda:  350.6094055175781\n",
            "SGLD accept prob:  0.9908456\n",
            "SGLD execution time:  32.47212839126587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:48<00:00, 28.68it/s, 1 steps of size 3.16e-03. acc. prob=0.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  358.1321105957031\n",
            "MALA execution time:  382.63348960876465\n",
            "Training set size: 31623, run 2\n",
            "x_train shape: (31623, 10), y_train shape: (31623, 10)\n",
            "Training data memory:  3794760\n",
            "SGLD lambda:  355.47833251953125\n",
            "SGLD accept prob:  0.99085\n",
            "SGLD execution time:  32.135979890823364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:47<00:00, 28.81it/s, 1 steps of size 3.16e-03. acc. prob=0.80]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  369.1020812988281\n",
            "MALA execution time:  381.5964562892914\n",
            "Training set size: 31623, run 3\n",
            "x_train shape: (31623, 10), y_train shape: (31623, 10)\n",
            "Training data memory:  3794760\n",
            "SGLD lambda:  342.5532531738281\n",
            "SGLD accept prob:  0.99009657\n",
            "SGLD execution time:  31.803086280822754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:46<00:00, 28.89it/s, 1 steps of size 3.16e-03. acc. prob=0.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  369.27911376953125\n",
            "MALA execution time:  381.2685797214508\n",
            "Training set size: 31623, run 4\n",
            "x_train shape: (31623, 10), y_train shape: (31623, 10)\n",
            "Training data memory:  3794760\n",
            "SGLD lambda:  353.2110900878906\n",
            "SGLD accept prob:  0.98845124\n",
            "SGLD execution time:  32.564656019210815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [05:48<00:00, 28.69it/s, 1 steps of size 3.16e-03. acc. prob=0.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  343.0865783691406\n",
            "MALA execution time:  384.88666677474976\n",
            "Training set size: 100000, run 0\n",
            "x_train shape: (100000, 10), y_train shape: (100000, 10)\n",
            "Training data memory:  12000000\n",
            "SGLD lambda:  357.1268005371094\n",
            "SGLD accept prob:  0.98993737\n",
            "SGLD execution time:  33.648396015167236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [15:44<00:00, 10.59it/s, 1 steps of size 3.16e-03. acc. prob=0.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  352.4219055175781\n",
            "MALA execution time:  1054.4247555732727\n",
            "Training set size: 100000, run 1\n",
            "x_train shape: (100000, 10), y_train shape: (100000, 10)\n",
            "Training data memory:  12000000\n",
            "SGLD lambda:  359.12408447265625\n",
            "SGLD accept prob:  0.9944546\n",
            "SGLD execution time:  32.02788949012756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [15:41<00:00, 10.62it/s, 1 steps of size 3.16e-03. acc. prob=0.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  345.0095520019531\n",
            "MALA execution time:  1052.1668529510498\n",
            "Training set size: 100000, run 2\n",
            "x_train shape: (100000, 10), y_train shape: (100000, 10)\n",
            "Training data memory:  12000000\n",
            "SGLD lambda:  362.73468017578125\n",
            "SGLD accept prob:  0.99624765\n",
            "SGLD execution time:  32.43241786956787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [15:39<00:00, 10.64it/s, 1 steps of size 3.16e-03. acc. prob=0.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  347.5590515136719\n",
            "MALA execution time:  1049.0516211986542\n",
            "Training set size: 100000, run 3\n",
            "x_train shape: (100000, 10), y_train shape: (100000, 10)\n",
            "Training data memory:  12000000\n",
            "SGLD lambda:  350.6711120605469\n",
            "SGLD accept prob:  0.9890411\n",
            "SGLD execution time:  33.73149871826172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [16:15<00:00, 10.25it/s, 1 steps of size 3.16e-03. acc. prob=0.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  355.29193115234375\n",
            "MALA execution time:  1088.828677892685\n",
            "Training set size: 100000, run 4\n",
            "x_train shape: (100000, 10), y_train shape: (100000, 10)\n",
            "Training data memory:  12000000\n",
            "SGLD lambda:  349.2572021484375\n",
            "SGLD accept prob:  0.9918579\n",
            "SGLD execution time:  33.226011514663696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 10000/10000 [15:46<00:00, 10.57it/s, 1 steps of size 3.16e-03. acc. prob=0.20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MALA lambda:  345.3625183105469\n",
            "MALA execution time:  1058.6680824756622\n"
          ]
        }
      ]
    }
  ]
}